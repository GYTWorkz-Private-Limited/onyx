# ğŸ“„ File Parser API (Docling & LLaMAParse)

A modular, production-ready FastAPI service that parses uploaded documents using either **Docling** or **LlamaParse**, returning both **plain text** and **Markdown** formats.

---

## âœ… Features

- **Modular Architecture**: Clean separation of concerns with organized folder structure
- **Multiple Parser Engines**: Choose between `docling` (open-source) or `llama` (cloud-based)
- **Wide Format Support**: `.pdf`, `.docx`, `.csv`, `.xlsx`, `.pptx`
- **Dual Output**: Plain text (`.txt`) and Markdown (`.md`) formats
- **RESTful API**: Well-documented endpoints with OpenAPI/Swagger
- **Type Safety**: Full Pydantic schema validation
- **Error Handling**: Comprehensive error handling and validation
- **Environment Configuration**: Flexible configuration via environment variables

---

## ğŸ—ï¸ Project Structure

```
file-parser/
â”œâ”€â”€ api/                    # API routes and endpoints
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ parse_routes.py     # Parse endpoint routes
â”œâ”€â”€ controllers/            # Business logic controllers
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ parse_controller.py # Parse operations controller
â”œâ”€â”€ models/                 # Data models and schemas
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ parse_models.py     # Parse result models
â”œâ”€â”€ services/               # Core parsing services
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_parser.py      # Abstract base parser
â”‚   â”œâ”€â”€ docling_service.py  # Docling parsing service
â”‚   â””â”€â”€ llamaparse_service.py # LlamaParse service
â”œâ”€â”€ schemas/                # Pydantic schemas
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ parse_schemas.py    # Request/response schemas
â”œâ”€â”€ tools/                  # Utility tools
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ file_validator.py   # File validation utilities
â”‚   â””â”€â”€ output_writer.py    # Output writing utilities
â”œâ”€â”€ utils/                  # General utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ constants.py        # Application constants
â”‚   â””â”€â”€ logging_config.py   # Logging configuration
â”œâ”€â”€ tests/                  # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py        # Test configuration
â”‚   â”œâ”€â”€ test_api.py        # API tests
â”‚   â””â”€â”€ test_services.py   # Service tests
â”œâ”€â”€ output/                 # Generated output files
â”œâ”€â”€ temp/                   # Temporary files (auto-created)
â”œâ”€â”€ logs/                   # Application logs
â”œâ”€â”€ environment.py          # Environment configuration
â”œâ”€â”€ server.py              # FastAPI application setup
â”œâ”€â”€ pyproject.toml         # Modern Python project config
â”œâ”€â”€ requirements.txt       # Dependencies (legacy)
â””â”€â”€ README.md             # This file
```

---

## ğŸ›  Setup

### 1. Clone and Navigate

```bash
git clone <your-repo-url>
cd file-parser
```

### 2. Create Virtual Environment

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

### 3. Install Dependencies

**Option A: Using pip (legacy)**
```bash
pip install -r requirements.txt
```

**Option B: Using pip with pyproject.toml (recommended)**
```bash
pip install -e .
```

### 4. Environment Configuration

Create a `.env` file in the project root:

```env
# LlamaParse Configuration
LLAMA_CLOUD_API_KEY=llx-<your-key>
PARSER_MODE=balanced
OUTPUT_FORMAT=markdown

# Application Configuration
OUTPUT_DIR=output
TEMP_DIR=temp
HOST=0.0.0.0
PORT=8000
DEBUG=false

# File Processing
MAX_FILE_SIZE=50000000
```

---

## ğŸš€ Running the Application

### Development Mode

```bash
uvicorn server:app --reload
```

### Production Mode

```bash
uvicorn server:app --host 0.0.0.0 --port 8000
```

### Using Python directly

```bash
python server.py
```

### Using Makefile

```bash
make run          # Development mode
make run-prod     # Production mode
```

Open your browser at: [http://localhost:8000/docs](http://localhost:8000/docs)

---

## ğŸ“š API Documentation

### Endpoints

- **POST `/api/v1/parse/`** - Parse uploaded file
- **GET `/api/v1/engines/`** - Get supported parser engines
- **GET `/api/v1/health/`** - Health check
- **GET `/docs`** - Interactive API documentation
- **GET `/redoc`** - Alternative API documentation

### Example Usage

```bash
# Using curl (Linux/Mac)
curl -X POST "http://localhost:8000/api/v1/parse/" \
     -H "accept: application/json" \
     -H "Content-Type: multipart/form-data" \
     -F "file=@document.pdf" \
     -F "engine=llama"

# Using PowerShell (Windows)
$form = @{
    file = Get-Item "document.pdf"
    engine = "llama"
}
Invoke-RestMethod -Uri "http://localhost:8000/api/v1/parse/" -Method Post -Form $form
```

---

## ğŸ“‚ Output Structure

For a file named `invoice.pdf`, you'll get:

```
output/
â”œâ”€â”€ invoice.txt      # Plain text extraction
â””â”€â”€ invoice.md       # Markdown formatted extraction
```

---

## ğŸ”§ Development

### Code Quality Tools

```bash
# Install development dependencies
pip install -e ".[dev]"

# Format code
make format

# Lint code
make lint

# Run tests
make test

# Run tests with coverage
make test-coverage

# Check everything
make check
```

### Adding New Parser Engines

1. Create a new service in `services/` inheriting from `BaseParser`
2. Implement the required abstract methods
3. Add the engine to `ParserEngine` enum in `schemas/parse_schemas.py`
4. Update the controller to handle the new engine

---

## ğŸ“ License

MIT License - see LICENSE file for details.

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

---

## ğŸ“ Support

For issues and questions, please open an issue on GitHub.
